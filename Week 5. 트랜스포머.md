### 트랜스포머



- 트랜스퍼 러닝
  - NN은 너무 커서 작은 데이터에서 오버핏이 발생
  - 이미지넷에서 NN으르 사용하여 학습하고 fine-tune을 진행하기
  - 사전학습된 큰모델에 새 층을 추가하여 훨씬 작은 데이터를 써도됨
- 워드를 벡터로 고치기
  - 원핫인코딩이 한 방법이 될 수 있다.
    - scales poorly
    - 너무나 sparse vector, 너무 많은 차원
    - 단어간 유사도를 알 수 없다
  - 임베딩을 통해 원핫을 dense vector로 활용
    - 이미 학습된 NLP task로 하는게 단어 임베딩에 효과적!
- N grams
  - N사이즈의 윈도우로 텍스트를 잘라서 마지막에 나올 단어를 예측하기위해 구성한 데이터셋
  - Skip grams : 타겟 단어의 양쪽을 보고 N-gram 샘플을 구성하게 함
- Word2Vec
- ELMO / ULMfit
  - 워드투벡과 글로브는 유명해졌지만..
    - 오로지 첫층만 모든 위키피디아를 보는 이익이 있었다.
    - 나머지 모델은 task dataset만 학습함
    - 문법을 학습하기?
  - GLUE
- Transformer
  - Attention
    - 텐서의 시퀀스 인풋
    - 각각에 가중치된 섬을 한 텐서의 시퀀스
  - Basic self-attention
    - 가중치 안배움. -> 그러면 가중치를 배워보자!
    - 시퀀스 순서가 계산 결과에 영향을 안줌
  - 쿼리, 키, 밸류
    - 모든 인풋 벡터는 3 방식으로 사용된다
      - own output y_i 의 어텐션 웨이트를 계산하기 위해 다른 모든 벡터와 비교된다.(쿼리)
      - output y_j 의 어텐션 웨이트 w_ij를 계산하기 위해 다른 모든 벡터와 비교된다.(키)
        - 어텐션 가중치합계의 결과를 형성하기 위해 다른 벡터와 합쳐짐(밸류)
  - 멀티헤드 어텐션
    - Wq,k,v 매트릭스의 서로다른 세트를 를 sumultaneously하게 배우는 것을 의미함.
  - 센프어텐션->레이어 정규화 -> Dense layer
    - 레이어 정규화
      - 인풋 벡터가 각각의 차원에서 uniform mean과 std를 가지고 있을 때 nn층이 가장 일을 잘함.
        - 인풋 스케일링과 가중치 initialization
      - mean과 variance를 reset함
    - 포지션 임베딩
      - 포지션에 의존하지 않는다. 
  - BERT, GPT-2, 디스틸BERT, T5
    - 어텐션 이즈 올 유 니드
      - 번역을 위한 인코더-디코더 모델
      - 마지막엔 인코더와 디코더로만 이뤄진 모델로 만들어짐 
    - GPT, -2
      - 다음 단어를 예측함. ELMO나 ULMFIT처럼.
    - BERT
      - 인코더 블록만 존재함. masking 없음
      - 엄청많은 tetxt에 대한 사전학습을 포함한다.
      - 340M 파라미터를 가짐
    - T5
      - 인풋과 아웃풋이 둘다 text string
      - C4에서 훈련됨(위키피디아보다 100배 큼)
    - GPT-3
      - 훈련데이터로 인해 예상치못한 '혐' 문장이 튀어나온다.
      - 자연어 설명을 넣어 코딩도 가능하고 엑셀 스크립트도 짤수있다.
    - DistillBERT
      - BERT의 97% 퍼포먼스
      - 명확하게 작고 빠르다.